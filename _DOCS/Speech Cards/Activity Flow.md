Complete System Flow Example
When a user says "go to the chair," our system processes this through multiple branches simultaneously. The AI branch converts speech to text, extracts the intent and target object using an LLM, while the vision pipeline detects the chair in the video frame and calculates its distance and position using depth detection and bounding boxes. The Central Hub integrates all this data into global variables - storing the intent, object type, distance of 3.2 meters, and screen coordinates. The Robotics branch uses PID control to calculate movement commands based on this data, sending the drone toward the chair. Meanwhile, the MR branch displays the processed video with bounding boxes, distance overlays, a minimap showing the planned path, and status updates - all in real-time as the drone executes the command.